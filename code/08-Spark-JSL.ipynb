{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZQ2ZrKZGbOi"
   },
   "source": [
    "# Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180182,
     "status": "ok",
     "timestamp": 1612746758649,
     "user": {
      "displayName": "Steven Doerstling",
      "photoUrl": "",
      "userId": "15104894116977045422"
     },
     "user_tz": 300
    },
    "id": "kW6Ip9aWF_Wr",
    "outputId": "21a2558b-a789-4c46-ce72-2bc002c565ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkNLP Version: 2.6.5\n",
      "SparkNLP-JSL Version: 2.7.2\n",
      "openjdk version \"11.0.9.1\" 2020-11-04\n",
      "OpenJDK Runtime Environment (build 11.0.9.1+1-Ubuntu-0ubuntu1.18.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.9.1+1-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 31kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 21.3MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting spark-nlp==2.6.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/1d/9a2a7c17fc3b3aa78b3921167feed4911d5a055833fea390e7741bba0870/spark_nlp-2.6.5-py2.py3-none-any.whl (130kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 4.0MB/s \n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-2.6.5\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/2.7.2-7ad44c2a1a61c48b6a74446b0a7cb6b97c58dba0\n",
      "Collecting spark-nlp-jsl==2.7.2\n",
      "\u001b[?25l  Downloading https://pypi.johnsnowlabs.com/2.7.2-7ad44c2a1a61c48b6a74446b0a7cb6b97c58dba0/spark-nlp-jsl/spark_nlp_jsl-2.7.2-py3-none-any.whl (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 422kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: spark-nlp==2.6.5 in /usr/local/lib/python3.6/dist-packages (from spark-nlp-jsl==2.7.2) (2.6.5)\n",
      "Installing collected packages: spark-nlp-jsl\n",
      "Successfully installed spark-nlp-jsl-2.7.2\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import spacy\n",
    "\n",
    "PATH_TO_LICENSE_KEY = ''\n",
    "\n",
    "#import license keys from drive\n",
    "with open(PATH_TO_LICENSE_KEY) as f:s\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "secret = license_keys['SECRET']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "sparknlp_version = license_keys[\"PUBLIC_VERSION\"]\n",
    "jsl_version = license_keys[\"JSL_VERSION\"]\n",
    "\n",
    "print ('SparkNLP Version:', sparknlp_version)\n",
    "print ('SparkNLP-JSL Version:', jsl_version)\n",
    "\n",
    "# Install Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==$sparknlp_version\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version --extra-index-url https://pypi.johnsnowlabs.com/$secret\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "\n",
    "spark = sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--4ukn3ZG6-N"
   },
   "source": [
    "# Define pipeline elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 795784,
     "status": "ok",
     "timestamp": 1612747374258,
     "user": {
      "displayName": "Steven Doerstling",
      "photoUrl": "",
      "userId": "15104894116977045422"
     },
     "user_tz": 300
    },
    "id": "Z8GSUtr2G88p",
    "outputId": "168bf02d-8ba3-4e19-b96c-ba1ffe0b4b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_jsl download started this may take some time.\n",
      "Approximate size to download 14 MB\n",
      "[OK!]\n",
      "sbiobert_base_cased_mli download started this may take some time.\n",
      "Approximate size to download 384.3 MB\n",
      "[OK!]\n",
      "sbiobertresolve_icd10cm_augmented download started this may take some time.\n",
      "Approximate size to download 1.2 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "  .setInputCol('text')\\\n",
    "  .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "  .setInputCols(['document'])\\\n",
    "  .setOutputCol('sentence')\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols(['sentence']) \\\n",
    "  .setOutputCol('token')\n",
    " \n",
    "word_embeddings_clinical = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_jsl = NerDLModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter_diagnosis = NerConverter() \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "  .setOutputCol(\"ner_chunk\")\\\n",
    "  .setWhiteList(['Diagnosis'])\n",
    "\n",
    "chunk_embeddings = ChunkEmbeddings()\\\n",
    "    .setInputCols([\"ner_chunk\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"chunk_embeddings\")\n",
    " \n",
    "c2doc = Chunk2Doc().setInputCols(\"ner_chunk\").setOutputCol(\"ner_chunk_doc\") \n",
    "\n",
    "sbiobert_embedder = BertSentenceEmbeddings\\\n",
    "  .pretrained(\"sbiobert_base_cased_mli\",'en','clinical/models')\\\n",
    "  .setInputCols([\"ner_chunk_doc\"])\\\n",
    "  .setOutputCol(\"sbert_embeddings\")\n",
    "\n",
    "sbert_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_icd10cm_augmented\",\"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"ner_chunk\", \"sbert_embeddings\"]) \\\n",
    "  .setOutputCol(\"icd10cm_code\")\\\n",
    "  .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "pipeline= Pipeline(\n",
    "    stages = [\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        word_embeddings_clinical,\n",
    "        ner_jsl,\n",
    "        ner_converter_diagnosis,\n",
    "        chunk_embeddings,\n",
    "        c2doc,\n",
    "        sbiobert_embedder,\n",
    "        sbert_resolver])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "light_pipeline = sparknlp.base.LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26JMoQAFIabW"
   },
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un1bjnoQIZJH"
   },
   "outputs": [],
   "source": [
    "def get_codes (light, code, text, url):\n",
    "\n",
    "  '''\n",
    "  example call: get_codes(light_pipeline, 'icd10cm_code', FEED_TEXT, FEED_URL)\n",
    "  '''\n",
    "\n",
    "  full_light_result = light.fullAnnotate(text)\n",
    "\n",
    "  urls = []\n",
    "  chunks = []\n",
    "  begin = []\n",
    "  end = []\n",
    "  sent = []\n",
    "  codes = []\n",
    "  results = []\n",
    "  resolutions = []\n",
    "  res_distances = []\n",
    "\n",
    "  for chunk, code in zip(full_light_result[0]['ner_chunk'], full_light_result[0][code]):\n",
    "      \n",
    "      urls.append(url)\n",
    "      chunks.append(chunk.result)\n",
    "      begin.append(chunk.begin)\n",
    "      end.append(chunk.end)\n",
    "      sent.append(chunk.metadata['sentence'])\n",
    "      codes.append(code.result) \n",
    "      results.append(code.metadata['all_k_results'])\n",
    "      resolutions.append(code.metadata['all_k_resolutions'])\n",
    "      res_distances.append(code.metadata['all_k_distances'])\n",
    "    \n",
    "\n",
    "  df = pd.DataFrame({'url':urls,\n",
    "                    'chunks':chunks, \n",
    "                     'begin': begin, \n",
    "                     'end':end, \n",
    "                     'sent':sent,\n",
    "                    'code':codes,\n",
    "                     'results':results,\n",
    "                    'resolutions':resolutions,\n",
    "                     'res_distances':res_distances})\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzrK11yrUw8t"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def run_pipeline(feed):\n",
    "  \n",
    "  r = []\n",
    "\n",
    "  for index, row in feed.iterrows():\n",
    "    url = row['url']\n",
    "    text = row['text']\n",
    "    er_results = get_codes(light_pipeline, 'icd10cm_code', text, url)\n",
    "    r.append(er_results)\n",
    "  \n",
    "  #return concatenated pandas dataframes\n",
    "  df = pd.concat(r)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxE_qPIQJu5u"
   },
   "source": [
    "# Import and process feed data\n",
    "\n",
    "- If you are running this on google colab (recommended), it might be best to export the feed data from gfm.db into a few .csv files based on runtime restrictions for your license.\n",
    "\n",
    "- Here, we exported the columns \"url\" and \"fund_description\" from all data into 4 separate .json files (~25k records in each file)\n",
    "\n",
    "Example code:\n",
    "\n",
    "```\n",
    "feed = feed[['url','fund_description']]\n",
    "feed.rename(columns={'fund_description':'text'}, inplace=True)\n",
    "dfs = np.array_split(feed, 4)\n",
    "\n",
    "PATH_TO_DATA_FOR_COLAB = ''\n",
    "\n",
    "for i in range(4):\n",
    "    with open(PATH_TO_DATA_FOR_COLAB + 'feed_chunk_' + str(i) + '.json', 'w', encoding='utf-8') as file:\n",
    "        dfs[i].to_json(file, orient=\"records\", force_ascii=False)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hr8jgTAAJz0V"
   },
   "outputs": [],
   "source": [
    "#read in chunk and analyze one at a time\n",
    "PATH_TO_CHUNK = ''\n",
    "chunk_n = 0\n",
    "\n",
    "with open(PATH_TO_CHUNK + 'feed_chunk_' + str(chunk_n) + '.json') as json_file:\n",
    "    feed = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797149,
     "status": "ok",
     "timestamp": 1612747380477,
     "user": {
      "displayName": "Steven Doerstling",
      "photoUrl": "",
      "userId": "15104894116977045422"
     },
     "user_tz": 300
    },
    "id": "8NAQyKVQDMX6",
    "outputId": "181774f3-5905-4c30-f029-351a97a45503"
   },
   "outputs": [],
   "source": [
    "feed = pd.DataFrame(feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz2Fx61UkvzO"
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM6qlnJCIx1d"
   },
   "source": [
    "The spark tokenizer does not work reliably to tokenize on punctuation without whitespace e.g. \"end.Beginning\"\n",
    "\n",
    "So will preprocess this manually to split tokens by .,!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQsGQqJAJhwz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def CustomTokenize(df):\n",
    "  r = []\n",
    "  for i in range(len(df)):\n",
    "    string = re.sub(r'(?<=[.,!\\\\?])(?=[^\\s])', r' ', df['text'][i])\n",
    "    r.append(string)\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcChk7b7MtRo"
   },
   "outputs": [],
   "source": [
    "feed.loc[:,'text_clean'] = CustomTokenize(feed)\n",
    "del feed['text']\n",
    "feed = feed.rename(columns={'text_clean':'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmuR0tYGV3gM"
   },
   "source": [
    "# Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2963160,
     "status": "ok",
     "timestamp": 1612808476758,
     "user": {
      "displayName": "Steven Doerstling",
      "photoUrl": "",
      "userId": "15104894116977045422"
     },
     "user_tz": 300
    },
    "id": "VCrN83P0V13p",
    "outputId": "4c52855e-57da-458d-fb52-86d49fbdf5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 41min 45s, sys: 24min 39s, total: 2h 6min 24s\n",
      "Wall time: 16h 58min 8s\n"
     ]
    }
   ],
   "source": [
    "%time r = run_pipeline(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1612808477899,
     "user": {
      "displayName": "Steven Doerstling",
      "photoUrl": "",
      "userId": "15104894116977045422"
     },
     "user_tz": 300
    },
    "id": "n1jy3NtgV-Lu"
   },
   "outputs": [],
   "source": [
    "EXPORT_PATH = ''\n",
    "r.to_csv(EXPORT_PATH + 'feed_chunk_' + str(chunk_n) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnOzANFsa_la"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOWBxihK6YxPU9dFdHLdFGt",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1hK_9v1NCXWRhyNCAg_9ewbNz6GoYMPKf",
   "name": "Spark-JSL-CCSR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
